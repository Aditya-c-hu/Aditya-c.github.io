<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Course Reflection</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #1a1a1a;
            color: #eeeeee;
            line-height: 1.8;
        }
        header {
            background: linear-gradient(135deg, #333333, #4d4d4d);
            color: #ffffff;
            text-align: center;
            padding: 30px 10px;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        header p {
            font-size: 1em;
            margin-top: 5px;
        }
        .container {
            margin: 20px auto;
            padding: 30px;
            max-width: 100%;
        }
        section {
            margin: 20px auto;
            padding: 30px;
            background-color: #333333;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.5);
        }
        h2 {
            font-size: 1.8em;
            margin-bottom: 15px;
            color: #ffffff;
            border-bottom: 2px solid #4d4d4d;
            padding-bottom: 5px;
        }
        p {
            font-size: 1em;
            margin-bottom: 15px;
            color: #cccccc;
        }
        ul {
            margin: 15px 0;
            padding-left: 20px;
            color: #cccccc;
        }
        li {
            margin-bottom: 10px;
            color: #cccccc;
        }
        strong {
            color: #ffffff;
        }
    </style>
</head>
<body>
    <header>
        <h1>Course Reflection</h1>
        <p>By Rohan Dixit</p>
    </header>
    <section>
        <h2>1. Iteration, Recursion, and Backtracking in Nature</h2>
        <p>
            <strong>Iteration:</strong> Iteration is evident in natural cycles that repeat over time. For instance, cell division involves mitosis, where cells continually divide.
        </p>
        <p>
            <strong>Recursion:</strong> Recursion is observed in self-similar natural structures. For example, trees demonstrate recursive branching patterns, with each branch splitting into smaller branches.
        </p>
        <p>
            <strong>Backtracking:</strong> Backtracking is seen in natural problem-solving processes, such as animals searching for food. When a path is unproductive, animals retrace their steps and try alternative routes.
        </p>
    </section>
    <section>
        <h2>2. Efficiency of Space and Time: Importance and Growth Orders</h2>
        <p>
            <strong>Definition and Significance:</strong><br>
            <ul>
                <li><strong>Time Efficiency:</strong> Measures the time an algorithm takes to solve a problem. It's essential to evaluate how well an algorithm scales with increasing input size.</li>
                <li><strong>Space Efficiency:</strong> Focuses on the memory an algorithm requires. For large datasets or systems with limited memory, selecting algorithms with minimal memory usage is crucial.</li>
            </ul>
            Choosing between time and space efficiency often involves trade-offs. For example, an algorithm that minimizes memory usage might take more time, while one that prioritizes speed could use more memory.
        </p>
        <p>
            <strong>Orders of Growth:</strong> The time complexity of an algorithm increases with the input size. Common growth rates include:
            <ul>
                <li><strong>O(1):</strong> Constant time; the algorithm's runtime is unaffected by input size (e.g., accessing an array element by index).</li>
                <li><strong>O(log n):</strong> Logarithmic growth; occurs in algorithms that halve the search space at each step (e.g., binary search).</li>
                <li><strong>O(n):</strong> Linear growth; runtime increases proportionally with input size (e.g., iterating through an array).</li>
                <li><strong>O(n log n):</strong> Linearithmic growth; found in efficient algorithms like merge sort and heapsort.</li>
                <li><strong>O(n²):</strong> Quadratic growth; typical in algorithms with nested loops (e.g., bubble sort).</li>
                <li><strong>O(2^n):</strong> Exponential growth; problem size doubles with each step (e.g., brute force solutions to problems like the traveling salesman problem).</li>
                <li><strong>O(n!):</strong> Factorial growth; seen in algorithms that evaluate all permutations (e.g., solving the traveling salesman problem through brute force).</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>3. Design Principles: Insights from Chapter 2</h2>
        <p>
            <ul>
                <li><strong>Decomposition:</strong> Breaking a problem into smaller, manageable parts. For example, in the zombie children problem from Lab 01, the solution was split into smaller tasks.</li>
                <li><strong>Pattern Recognition:</strong> Identifying recurring structures in problems. For instance, Indian fort designs often follow repeated patterns in layout and defense mechanisms.</li>
                <li><strong>Abstraction:</strong> Focusing on key features while ignoring irrelevant details. In Lab 01, object shapes were abstracted to emphasize size, position, and behavior.</li>
                <li><strong>Cautious and Bold Travel:</strong> These refer to graph traversal strategies: <strong>DFS</strong> (Depth-First Search) dives deep into a graph's nodes, while <strong>BFS</strong> (Breadth-First Search) carefully explores breadth-wise.</li>
                <li><strong>Pruning:</strong> Cutting off unproductive branches of computation. In the N-Queens problem, invalid placements are eliminated early to avoid unnecessary calculations.</li>
                <li><strong>Memoization and Pre-computation:</strong> Storing intermediate results to avoid redundant work. For instance, storing previously computed Fibonacci numbers.</li>
                <li><strong>Lazy Propagation:</strong> Delaying updates until necessary. This technique is used in data structures like segment trees, where changes are applied only when needed.</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>4. Trees and Optimization Techniques</h2>
        <p>
            <ul>
                <li><strong>Binary Trees:</strong> Simple structures where each node has at most two children. They are used for basic hierarchies but lack self-balancing.</li>
                <li><strong>Binary Search Trees (BSTs):</strong> Binary trees with ordering properties, allowing for efficient searching, insertion, and deletion. However, they can become unbalanced.</li>
                <li><strong>AVL Trees:</strong> Self-balancing binary search trees that maintain O(log n) height, improving efficiency through rotations.</li>
                <li><strong>Red-Black Trees:</strong> Balanced trees similar to AVL but use color-coding to maintain balance, requiring fewer rotations and offering better performance in practice.</li>
                <li><strong>Tries:</strong> Specialized for string operations, especially for dictionary and prefix-based queries, by organizing data in a tree where nodes represent common prefixes.</li>
                <li><strong>Heaps:</strong> Efficient priority queues where the largest or smallest element can be accessed in O(1) time, with inserts and deletions occurring in O(log n) time.</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>5. Array Query Algorithms</h2>
        <p>
            Efficient data structures and algorithms can optimize common array operations:
            <ul>
                <li><strong>Lookup Table:</strong> Using a precomputed table for fast lookups, reducing time complexity to O(1) but requiring more memory.</li>
                <li><strong>Segment Trees:</strong> Provide efficient range queries and updates in O(log n) time, especially useful for problems that require repeated range queries.</li>
                <li><strong>Sparse Table:</strong> An array preprocessed for constant-time range queries, ideal for static data where no updates are necessary.</li>
                <li><strong>Fenwick Trees:</strong> Used to compute prefix sums and provide efficient range queries and updates in logarithmic time.</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>6. Trees vs. Graphs: Key Differences and Applications</h2>
        <table>
            <tr>
                <th>Aspect</th>
                <th>Tree</th>
                <th>Graph</th>
            </tr>
            <tr>
                <td>Structure</td>
                <td>Hierarchical, no cycles</td>
                <td>Can be cyclic or acyclic</td>
            </tr>
            <tr>
                <td>Connections</td>
                <td>Each node has one parent (except root)</td>
                <td>Nodes can have multiple connections to other nodes (edges can be directed or undirected)</td>
            </tr>
            <tr>
                <td>Root Node</td>
                <td>There is a single root</td>
                <td>No single root; any node can be the starting point</td>
            </tr>
            <tr>
                <td>Traversal</td>
                <td>Preorder, Inorder, Postorder, Level-order</td>
                <td>DFS, BFS, Topological sort, etc.</td>
            </tr>
            <tr>
                <td>Applications</td>
                <td>File systems, expression trees, organizational charts</td>
                <td>Social networks, routing, dependencies, circuit designs</td>
            </tr>
            <tr>
                <td>Memory Complexity</td>
                <td>Linear in terms of nodes (O(n))</td>
                <td>Can be more complex depending on the number of edges (O(n + m))</td>
            </tr>
        </table>
    </section>
    <section>
        <h2>7. Sorting and Searching Algorithms</h2>
        <p>
            <strong>Sorting Algorithms:</strong>
            <ul>
                <li><strong>Bubble Sort:</strong> Simple but inefficient with O(n²) time complexity, primarily used for educational purposes.</li>
                <li><strong>Quick Sort:</strong> Efficient with average O(n log n) complexity. It uses a divide-and-conquer approach and is typically faster than merge sort in practice, though it may degrade to O(n²) in the worst case.</li>
                <li><strong>Heap Sort:</strong> Also O(n log n) but generally slower than quicksort because of the overhead of maintaining the heap structure.</li>
            </ul>
            <strong>Searching Algorithms:</strong>
            <ul>
                <li><strong>Linear Search:</strong> Checks every element in the array, with a time complexity of O(n). It's simple but inefficient for large datasets.</li>
                <li><strong>Binary Search:</strong> Assumes a sorted array and reduces the search space in half each time, with O(log n) time complexity.</li>
                <li><strong>Hashing:</strong> Uses a hash function to directly map keys to values for O(1) average lookup time.</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>8. Graph Algorithms: Spanning Trees and Shortest Paths</h2>
        <p>
            <strong>Spanning Trees:</strong> A spanning tree connects all vertices in a graph with the minimum number of edges.
            <ul>
                <li><strong>Prim’s Algorithm:</strong> Starts with a node and grows the minimum spanning tree (MST) by adding the smallest edge connecting a vertex in the tree to a vertex outside the tree.</li>
                <li><strong>Kruskal’s Algorithm:</strong> Sorts edges by weight and adds them to the MST, ensuring no cycles are formed. This is often more efficient in sparse graphs.</li>
            </ul>
            <strong>Shortest Path Algorithms:</strong> These algorithms find the shortest path from a starting vertex to all other vertices in a graph.
            <ul>
                <li><strong>Dijkstra’s Algorithm:</strong> Efficient for graphs with non-negative weights, finding the shortest path in O(V + E log V) time using a priority queue.</li>
                <li><strong>Bellman-Ford Algorithm:</strong> Can handle negative edge weights and detect negative weight cycles, with a time complexity of O(VE).</li>
                <li><strong>Floyd-Warshall Algorithm:</strong> Computes all-pairs shortest paths, but with O(V³) complexity, making it suitable for smaller graphs.</li>
            </ul>
        </p>
    </section>
    <section>
        <h2>9. Algorithm Design Techniques</h2>
        <ul>
            <li><strong>Divide and Conquer:</strong> Break problems into smaller sub-problems. Example: Merge Sort divides the array into halves and recursively sorts them before combining them.</li>
            <li><strong>Dynamic Programming:</strong> Store solutions to sub-problems to avoid redundant work. Example: Fibonacci series, where each value is computed once and stored for future use.</li>
            <li><strong>Greedy:</strong> Make locally optimal choices to achieve a globally optimal solution. Example: The Huffman coding algorithm minimizes the total weight of encoded symbols.</li>
            <li><strong>Backtracking:</strong> Explore all possibilities and retract paths when a solution is not feasible. Example: Solving the N-Queens problem, where the algorithm backtracks and tries different placements.</li>
            <li><strong>Branch and Bound:</strong> Prune ineffective paths early to save computation. Example: The traveling salesman problem, where paths that exceed the current best known path are abandoned early.</li>
            <li><strong>Randomized Algorithms:</strong> Introduce randomness for simplicity or efficiency. Example: QuickSort, where the pivot is chosen randomly to improve performance.</li>
        </ul>
    </section>
</body>
</html>
